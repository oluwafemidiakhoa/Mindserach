{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oluwafemidiakhoa/Mindserach/blob/master/AI_Researcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK9RsXFj7me1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install langchain langchain-community\n",
        "%pip install langchainhub\n",
        "%pip install langchain-chroma\n",
        "%pip install langchain-groq\n",
        "%pip install langchain-huggingface\n",
        "%pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9VDbklT7u1q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import langchain\n",
        "import chromadb\n",
        "from groq import Groq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import xml.etree.ElementTree as ET\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import CSVLoader, DirectoryLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_groq import ChatGroq  # Replaced ChatGroq with ChatOpenAI as ChatGroq doesn't exist\n",
        "import gradio as gr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtGYPYTg74wb"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "client = Groq(\n",
        "   api_key=groq_api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GObEN0pR9n2s",
        "outputId": "aba32471-ba60-4107-ea11-52395e067a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded abstract for paper 2301.10045\n",
            "Downloaded abstract for paper 2301.10046\n",
            "Downloaded abstract for paper 2301.10047\n",
            "Downloaded abstract for paper 2301.10048\n",
            "Downloaded abstract for paper 2301.10049\n"
          ]
        }
      ],
      "source": [
        "# Create a directory to store the abstracts\n",
        "os.makedirs('ai_papers', exist_ok=True)\n",
        "\n",
        "# List of arXiv paper IDs to download abstracts for\n",
        "paper_ids = [\n",
        "    '2301.10045',\n",
        "    '2301.10046',\n",
        "    '2301.10047',\n",
        "    '2301.10048',\n",
        "    '2301.10049',\n",
        "    # Add more paper IDs as needed\n",
        "]\n",
        "\n",
        "# Function to download abstracts\n",
        "def download_abstracts(paper_ids):\n",
        "    for paper_id in paper_ids:\n",
        "        url = f'https://export.arxiv.org/api/query?id_list={paper_id}'\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            content = response.text\n",
        "            # Extract the abstract using XML parsing\n",
        "            root = ET.fromstring(content)\n",
        "            summary = root.find('.//{http://www.w3.org/2005/Atom}summary')\n",
        "            if summary is not None:\n",
        "                abstract = summary.text.strip()\n",
        "                # Save the abstract to a text file\n",
        "                with open(f'ai_papers/{paper_id}.txt', 'w') as f:\n",
        "                    f.write(abstract)\n",
        "                print(f'Downloaded abstract for paper {paper_id}')\n",
        "            else:\n",
        "                print(f'Abstract not found for paper {paper_id}')\n",
        "        else:\n",
        "            print(f'Failed to download paper {paper_id}')\n",
        "\n",
        "download_abstracts(paper_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT6Bfo4knM5R",
        "outputId": "7c9bc59d-33ef-49ed-bde6-d9297ba7ff1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 5 documents.\n"
          ]
        }
      ],
      "source": [
        "loader = DirectoryLoader('ai_papers', glob='*.txt', loader_cls=TextLoader)\n",
        "data = loader.load()\n",
        "print(f'Loaded {len(data)} documents.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10botytcrU5v",
        "outputId": "dc6799cb-467d-47a0-d797-dddc46d41576"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embed_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2UOebaEsGsB"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=data,\n",
        "    embedding=embed_model,\n",
        "    persist_directory='/content/ai_papers_vectorstore',\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LIFBple9Slc"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gqTARic1E4R"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are an AI research assistant.\n",
        "Use the provided context to answer the question.\n",
        "If you don't know the answer, say so. Provide a detailed explanation.\n",
        "Do not mention the context in your answer.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R4OclCF3lbz"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", api_key=groq_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovb_ebxV1Lfk"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drg4s6CU1azo",
        "outputId": "ce29e334-2e0b-4360-cdd2-a88f4318e0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are recent advancements in transformer architectures specifically in the context of video inpainting. One such advancement is the flow-guided transformer (FGT) architecture, which aims to address the issue of query degradation in the multi-head self-attention (MHSA) mechanism. \n",
            "\n",
            "The FGT architecture has been further improved to create FGT++, which includes several key advancements. Firstly, a lightweight flow completion network is designed using local aggregation and edge loss. \n",
            "\n",
            "Secondly, a flow guidance feature integration module is proposed, which uses motion discrepancy to enhance features, along with a flow-guided feature propagation module that warps features according to the flows. \n",
            "\n",
            "Lastly, the transformer is decoupled along the temporal and spatial dimensions, where flows are used to select tokens through a temporally deformable MHSA mechanism, and global tokens are combined with inner-window local tokens through a dual perspective MHSA mechanism. \n",
            "\n",
            "These advancements have been experimentally evaluated to show that FGT++ outperforms existing video inpainting networks both qualitatively and quantitatively.\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke(\"What are the recent advancements in transformer architectures?\")\n",
        "print(response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "_BbujuR4352Q",
        "outputId": "b3332f44-00eb-4a35-8ca5-09cc80cf58ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://35875db43ff8a9334f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://35875db43ff8a9334f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def rag_memory_stream(text):\n",
        "    partial_text = \"\"\n",
        "    for new_text in rag_chain.stream(text):\n",
        "        partial_text += new_text\n",
        "        yield partial_text\n",
        "\n",
        "title = \"AI Research Assistant with Groq API and LangChain\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=rag_memory_stream,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=title,\n",
        "    description=\"Ask questions about recent AI research papers.\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWfM7r+NgJwjuSwZWPLddQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}